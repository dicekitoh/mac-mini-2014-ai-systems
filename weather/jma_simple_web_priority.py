#!/usr/bin/env python3
"""
æ°—è±¡åºè­¦å ±ãƒ»æ³¨æ„å ± ç°¡æ˜“Webå„ªå…ˆã‚·ã‚¹ãƒ†ãƒ 
å®Ÿéš›ã®Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã¨APIã‚’æ¯”è¼ƒã—ã¦ã€ç›¸é•ãŒã‚ã‚Œã°Webæƒ…å ±ã‚’å„ªå…ˆè¡¨ç¤º
"""

import requests
import json
from datetime import datetime
import re
from bs4 import BeautifulSoup

class JMASimpleWebPriority:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.area_codes = {
            '016000': 'çŸ³ç‹©åœ°æ–¹',
            '015000': 'ç©ºçŸ¥åœ°æ–¹'
        }
    
    def get_web_page_content(self, area_code):
        """Webãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—"""
        area_name = self.area_codes[area_code]
        url = f"https://www.jma.go.jp/bosai/warning/#area_type=class20s&area_code={area_code}"
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            page_text = soup.get_text()
            
            # ãƒšãƒ¼ã‚¸ã‹ã‚‰è­¦å ±é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
            warning_keywords = ['è­¦å ±', 'æ³¨æ„å ±', 'æ¿ƒéœ§', 'å¼·é¢¨', 'å¤§é›¨', 'å¤§é›ª', 'æš´é¢¨', 'é›·']
            found_keywords = {}
            
            for keyword in warning_keywords:
                count = page_text.count(keyword)
                if count > 0:
                    found_keywords[keyword] = count
            
            # æ™‚åˆ»æƒ…å ±ã‚’æ¤œç´¢
            time_matches = re.findall(r'\d{1,2}æœˆ\d{1,2}æ—¥\d{1,2}æ™‚\d{1,2}åˆ†', page_text)
            
            return {
                'area_name': area_name,
                'area_code': area_code,
                'source': 'web',
                'url': url,
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'keywords_found': found_keywords,
                'time_matches': time_matches,
                'page_title': soup.title.get_text(strip=True) if soup.title else 'ä¸æ˜',
                'total_keywords': sum(found_keywords.values()) if found_keywords else 0
            }
            
        except Exception as e:
            return {
                'area_name': area_name,
                'area_code': area_code,
                'source': 'web',
                'error': str(e),
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    def get_api_data(self, area_code):
        """APIã‹ã‚‰è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        area_name = self.area_codes[area_code]
        api_url = f"https://www.jma.go.jp/bosai/warning/data/warning/{area_code}.json"
        
        try:
            response = self.session.get(api_url, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            
            # APIã‹ã‚‰è­¦å ±æƒ…å ±ã‚’æŠ½å‡º
            warnings_summary = []
            total_warnings = 0
            
            for area_type in data.get('areaTypes', []):
                for area in area_type.get('areas', []):
                    for warning in area.get('warnings', []):
                        warning_name = warning.get('name', 'ä¸æ˜')
                        warnings_summary.append(warning_name)
                        total_warnings += 1
            
            return {
                'area_name': area_name,
                'area_code': area_code,
                'source': 'api',
                'api_url': api_url,
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'report_datetime': data.get('reportDatetime', 'ä¸æ˜'),
                'warnings_summary': warnings_summary,
                'total_warnings': total_warnings,
                'raw_data': data
            }
            
        except Exception as e:
            return {
                'area_name': area_name,
                'area_code': area_code,
                'source': 'api',
                'error': str(e),
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    def compare_and_prioritize(self, web_data, api_data):
        """Webã¨APIã‚’æ¯”è¼ƒã—ã¦Webå„ªå…ˆã§çµ±åˆ"""
        area_name = web_data.get('area_name', api_data.get('area_name', 'ä¸æ˜'))
        
        comparison = {
            'area_name': area_name,
            'comparison_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'web_status': 'success' if 'error' not in web_data else 'error',
            'api_status': 'success' if 'error' not in api_data else 'error',
            'priority_source': 'web',
            'final_data': {},
            'differences_detected': False
        }
        
        # Webæƒ…å ±ã®å‡¦ç†
        if 'error' not in web_data:
            web_keywords = web_data.get('total_keywords', 0)
            web_time = web_data.get('time_matches', [])
            
            comparison['final_data'].update({
                'primary_source': 'web',
                'web_keywords_count': web_keywords,
                'web_time_info': web_time,
                'web_found_keywords': web_data.get('keywords_found', {}),
                'web_access_time': web_data.get('access_time')
            })
        
        # APIæƒ…å ±ã®å‡¦ç†
        if 'error' not in api_data:
            api_warnings = api_data.get('total_warnings', 0)
            api_time = api_data.get('report_datetime', 'ä¸æ˜')
            
            comparison['final_data'].update({
                'api_warnings_count': api_warnings,
                'api_report_time': api_time,
                'api_warnings_list': api_data.get('warnings_summary', []),
                'api_access_time': api_data.get('access_time')
            })
            
            # å·®ç•°æ¤œå‡º
            web_keywords = web_data.get('total_keywords', 0) if 'error' not in web_data else 0
            if web_keywords != api_warnings:
                comparison['differences_detected'] = True
                comparison['difference_details'] = f"Web: {web_keywords}ä»¶ vs API: {api_warnings}ä»¶"
        
        # Webå„ªå…ˆã®æœ€çµ‚åˆ¤å®š
        if comparison['web_status'] == 'success':
            comparison['final_data']['recommended_source'] = 'web'
            comparison['final_data']['reason'] = 'Webæƒ…å ±ã‚’å„ªå…ˆä½¿ç”¨'
        elif comparison['api_status'] == 'success':
            comparison['final_data']['recommended_source'] = 'api'
            comparison['final_data']['reason'] = 'Webã‚¨ãƒ©ãƒ¼ã®ãŸã‚APIä½¿ç”¨'
        else:
            comparison['final_data']['recommended_source'] = 'none'
            comparison['final_data']['reason'] = 'ä¸¡æ–¹ã¨ã‚‚ã‚¨ãƒ©ãƒ¼'
        
        return comparison
    
    def run_analysis(self):
        """ãƒ¡ã‚¤ãƒ³åˆ†æå®Ÿè¡Œ"""
        print("ğŸŒ¤ï¸  æ°—è±¡åºè­¦å ±ãƒ»æ³¨æ„å ± Webå„ªå…ˆåˆ†æã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 50)
        
        results = {
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'priority_policy': 'web_first',
            'areas': {}
        }
        
        for area_code, area_name in self.area_codes.items():
            print(f"\nğŸ“ {area_name} åˆ†æä¸­...")
            
            # Webãƒ‡ãƒ¼ã‚¿å–å¾—
            print(f"  ğŸŒ Webãƒšãƒ¼ã‚¸è§£æ...")
            web_data = self.get_web_page_content(area_code)
            
            # APIãƒ‡ãƒ¼ã‚¿å–å¾—
            print(f"  ğŸ”§ APIå–å¾—...")
            api_data = self.get_api_data(area_code)
            
            # æ¯”è¼ƒãƒ»çµ±åˆ
            print(f"  âš–ï¸  Webå„ªå…ˆçµ±åˆ...")
            comparison = self.compare_and_prioritize(web_data, api_data)
            
            results['areas'][area_code] = {
                'web_data': web_data,
                'api_data': api_data,
                'comparison': comparison
            }
            
            # çµæœè¡¨ç¤º
            self.display_area_result(comparison)
        
        # çµæœä¿å­˜
        filename = f"jma_web_priority_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ çµæœä¿å­˜: {filename}")
        print(f"ğŸ¯ Webå„ªå…ˆåˆ†æå®Œäº†")
        
        return results
    
    def display_area_result(self, comparison):
        """åœ°åŸŸåˆ¥çµæœã‚’è¡¨ç¤º"""
        area_name = comparison['area_name']
        final_data = comparison['final_data']
        
        print(f"    ğŸ“Š {area_name} çµæœ:")
        print(f"       æ¨å¥¨ã‚½ãƒ¼ã‚¹: {final_data.get('recommended_source', 'unknown')}")
        print(f"       ç†ç”±: {final_data.get('reason', 'unknown')}")
        
        if comparison['differences_detected']:
            print(f"       âš ï¸  å·®ç•°æ¤œå‡º: {comparison.get('difference_details', 'unknown')}")
        else:
            print(f"       âœ… Web/APIä¸€è‡´")
        
        # Webæƒ…å ±
        if 'web_keywords_count' in final_data:
            web_count = final_data['web_keywords_count']
            web_keywords = final_data.get('web_found_keywords', {})
            print(f"       ğŸŒ Web: {web_count}ä»¶ {list(web_keywords.keys())}")
        
        # APIæƒ…å ±
        if 'api_warnings_count' in final_data:
            api_count = final_data['api_warnings_count']
            api_time = final_data.get('api_report_time', 'ä¸æ˜')
            print(f"       ğŸ”§ API: {api_count}ä»¶ ç™ºè¡¨æ™‚åˆ»:{api_time}")

if __name__ == "__main__":
    analyzer = JMASimpleWebPriority()
    result = analyzer.run_analysis()