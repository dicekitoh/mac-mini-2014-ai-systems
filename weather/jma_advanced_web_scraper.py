#!/usr/bin/env python3
"""
æ°—è±¡åºè­¦å ±ãƒ»æ³¨æ„å ±ã‚µã‚¤ãƒˆ é«˜åº¦Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ V2
JavaScriptã§å‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã¦å–å¾—
"""

import requests
import json
from datetime import datetime
import re
from bs4 import BeautifulSoup
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class JMAAdvancedWebScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # åœ°åŸŸã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ”ãƒ³ã‚°
        self.area_codes = {
            '016000': 'çŸ³ç‹©åœ°æ–¹',
            '015000': 'ç©ºçŸ¥åœ°æ–¹'
        }
    
    def setup_selenium_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’è¨­å®š"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        
        try:
            driver = webdriver.Chrome(options=chrome_options)
            return driver
        except Exception as e:
            print(f"âš ï¸ Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼åˆæœŸåŒ–å¤±æ•—: {e}")
            return None
    
    def scrape_with_selenium(self, area_code):
        """Seleniumã‚’ä½¿ç”¨ã—ã¦JavaScriptå®Ÿè¡Œå¾Œã®ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        area_name = self.area_codes[area_code]
        print(f"ğŸ” {area_name} Seleniumé«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œä¸­...")
        
        driver = self.setup_selenium_driver()
        if not driver:
            return None
        
        try:
            # åœ°åŸŸåˆ¥ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            url = f"https://www.jma.go.jp/bosai/warning/#area_type=class20s&area_code={area_code}"
            driver.get(url)
            
            # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # è¿½åŠ ã§å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            time.sleep(3)
            
            # ãƒšãƒ¼ã‚¸ã‚½ãƒ¼ã‚¹ã‚’å–å¾—
            page_source = driver.page_source
            
            # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # è­¦å ±ãƒ»æ³¨æ„å ±è¦ç´ ã‚’è©³ç´°æ¤œç´¢
            warning_data = self.extract_warnings_from_html(soup, area_name)
            
            # ç™ºè¡¨æ™‚åˆ»ã‚’è©³ç´°æ¤œç´¢
            time_data = self.extract_time_info_from_html(soup)
            
            result = {
                'area_name': area_name,
                'area_code': area_code,
                'method': 'selenium',
                'url': url,
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'page_title': soup.title.get_text(strip=True) if soup.title else 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜',
                'warnings': warning_data,
                'time_info': time_data,
                'full_text_keywords': self.search_keywords_in_text(page_source)
            }
            
            driver.quit()
            return result
            
        except Exception as e:
            print(f"âŒ {area_name} Seleniumã‚¨ãƒ©ãƒ¼: {str(e)}")
            if driver:
                driver.quit()
            return {
                'area_name': area_name,
                'area_code': area_code,
                'method': 'selenium',
                'error': str(e),
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    def extract_warnings_from_html(self, soup, area_name):
        """HTMLã‹ã‚‰è­¦å ±ãƒ»æ³¨æ„å ±ã‚’è©³ç´°æŠ½å‡º"""
        warnings = []
        
        # å„ç¨®è­¦å ±ãƒ»æ³¨æ„å ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢
        warning_keywords = [
            'æš´é¢¨è­¦å ±', 'å¤§é›¨è­¦å ±', 'æ´ªæ°´è­¦å ±', 'å¤§é›ªè­¦å ±', 'æš´é¢¨é›ªè­¦å ±',
            'æ¿ƒéœ§æ³¨æ„å ±', 'é›·æ³¨æ„å ±', 'ä¹¾ç‡¥æ³¨æ„å ±', 'ãªã ã‚Œæ³¨æ„å ±', 'ç€æ°·æ³¨æ„å ±',
            'ç€é›ªæ³¨æ„å ±', 'èé›ªæ³¨æ„å ±', 'éœœæ³¨æ„å ±', 'ä½æ¸©æ³¨æ„å ±', 'å¼·é¢¨æ³¨æ„å ±',
            'å¤§é›¨æ³¨æ„å ±', 'æ´ªæ°´æ³¨æ„å ±', 'å¤§é›ªæ³¨æ„å ±', 'é¢¨é›ªæ³¨æ„å ±'
        ]
        
        # è­¦å ±ãƒ»æ³¨æ„å ±é–¢é€£ã®ã‚¯ãƒ©ã‚¹åã‚„ID
        warning_selectors = [
            '[class*="warning"]', '[class*="alert"]', '[class*="caution"]',
            '[id*="warning"]', '[id*="alert"]', '[class*="weather"]',
            '[class*="meteorological"]'
        ]
        
        # å„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§è¦ç´ ã‚’æ¤œç´¢
        for selector in warning_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if text and len(text) > 0:
                    # è­¦å ±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    for keyword in warning_keywords:
                        if keyword in text:
                            warnings.append({
                                'type': 'detected_warning',
                                'keyword': keyword,
                                'text': text,
                                'selector': selector,
                                'element_tag': element.name
                            })
        
        # ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã‹ã‚‰ã‚‚è­¦å ±ã‚’æ¤œç´¢
        full_text = soup.get_text()
        for keyword in warning_keywords:
            if keyword in full_text:
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‰å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                matches = re.finditer(re.escape(keyword), full_text)
                for match in matches:
                    start = max(0, match.start() - 50)
                    end = min(len(full_text), match.end() + 50)
                    context = full_text[start:end].strip()
                    
                    warnings.append({
                        'type': 'text_search',
                        'keyword': keyword,
                        'context': context,
                        'position': match.start()
                    })
        
        return warnings
    
    def extract_time_info_from_html(self, soup):
        """HTMLã‹ã‚‰æ™‚åˆ»æƒ…å ±ã‚’è©³ç´°æŠ½å‡º"""
        time_info = []
        
        # æ™‚åˆ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        time_patterns = [
            r'\d{1,2}æœˆ\d{1,2}æ—¥\d{1,2}æ™‚\d{1,2}åˆ†',
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥\d{1,2}æ™‚\d{1,2}åˆ†',
            r'\d{1,2}æ—¥\d{1,2}æ™‚\d{1,2}åˆ†',
            r'ä»¤å’Œ\d+å¹´\d{1,2}æœˆ\d{1,2}æ—¥\d{1,2}æ™‚\d{1,2}åˆ†'
        ]
        
        full_text = soup.get_text()
        
        for pattern in time_patterns:
            matches = re.finditer(pattern, full_text)
            for match in matches:
                start = max(0, match.start() - 30)
                end = min(len(full_text), match.end() + 30)
                context = full_text[start:end].strip()
                
                time_info.append({
                    'time_text': match.group(),
                    'pattern': pattern,
                    'context': context,
                    'position': match.start()
                })
        
        return time_info
    
    def search_keywords_in_text(self, text):
        """ãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢"""
        keywords = {
            'çŸ³ç‹©': text.count('çŸ³ç‹©'),
            'ç©ºçŸ¥': text.count('ç©ºçŸ¥'),
            'è­¦å ±': text.count('è­¦å ±'),
            'æ³¨æ„å ±': text.count('æ³¨æ„å ±'),
            'ç™ºè¡¨': text.count('ç™ºè¡¨'),
            'è§£é™¤': text.count('è§£é™¤'),
            'ç¶™ç¶š': text.count('ç¶™ç¶š'),
            'æ¿ƒéœ§': text.count('æ¿ƒéœ§'),
            'å¼·é¢¨': text.count('å¼·é¢¨'),
            'å¤§é›¨': text.count('å¤§é›¨')
        }
        
        return {k: v for k, v in keywords.items() if v > 0}
    
    def get_api_data_detailed(self, area_code):
        """APIãƒ‡ãƒ¼ã‚¿ã®è©³ç´°å–å¾—"""
        area_name = self.area_codes[area_code]
        api_url = f"https://www.jma.go.jp/bosai/warning/data/warning/{area_code}.json"
        
        try:
            response = self.session.get(api_url, timeout=10)
            response.raise_for_status()
            
            raw_data = response.json()
            
            # APIãƒ‡ãƒ¼ã‚¿ã®è©³ç´°è§£æ
            parsed_warnings = []
            report_time = raw_data.get('reportDatetime', 'unknown')
            
            for area_type in raw_data.get('areaTypes', []):
                for area in area_type.get('areas', []):
                    area_info = {
                        'area_name': area.get('name', 'unknown'),
                        'area_code': area.get('code', 'unknown'),
                        'warnings': []
                    }
                    
                    for warning in area.get('warnings', []):
                        warning_info = {
                            'name': warning.get('name', 'unknown'),
                            'code': warning.get('code', 'unknown'),
                            'status': warning.get('status', 'unknown')
                        }
                        area_info['warnings'].append(warning_info)
                    
                    if area_info['warnings']:  # è­¦å ±ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ 
                        parsed_warnings.append(area_info)
            
            return {
                'area_name': area_name,
                'area_code': area_code,
                'method': 'api',
                'api_url': api_url,
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'report_datetime': report_time,
                'warnings': parsed_warnings,
                'raw_data': raw_data
            }
            
        except Exception as e:
            return {
                'area_name': area_name,
                'area_code': area_code,
                'method': 'api',
                'error': str(e),
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    def run_comprehensive_analysis(self):
        """åŒ…æ‹¬çš„ãªè­¦å ±ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        print("ğŸŒ¤ï¸  æ°—è±¡åºè­¦å ±ãƒ»æ³¨æ„å ± é«˜åº¦åˆ†æã‚·ã‚¹ãƒ†ãƒ  V2")
        print("=" * 60)
        
        results = {
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'method': 'comprehensive',
            'areas': {}
        }
        
        for area_code, area_name in self.area_codes.items():
            print(f"\nğŸ“ {area_name} ({area_code}) åˆ†æé–‹å§‹")
            
            area_result = {
                'area_name': area_name,
                'area_code': area_code,
                'web_data': None,
                'api_data': None,
                'final_recommendation': None
            }
            
            # Seleniumã«ã‚ˆã‚‹é«˜åº¦Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            print(f"  ğŸ” Webé«˜åº¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ...")
            web_data = self.scrape_with_selenium(area_code)
            area_result['web_data'] = web_data
            
            # APIè©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—
            print(f"  ğŸ”§ APIè©³ç´°ãƒ‡ãƒ¼ã‚¿å–å¾—...")
            api_data = self.get_api_data_detailed(area_code)
            area_result['api_data'] = api_data
            
            # çµæœæ¯”è¼ƒãƒ»åˆ†æ
            final_rec = self.analyze_and_recommend(web_data, api_data)
            area_result['final_recommendation'] = final_rec
            
            results['areas'][area_code] = area_result
            
            print(f"  âœ… {area_name} åˆ†æå®Œäº†")
        
        # çµæœã‚’ä¿å­˜
        filename = f"jma_comprehensive_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è©³ç´°çµæœã‚’ä¿å­˜: {filename}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self.display_summary(results)
        
        return results
    
    def analyze_and_recommend(self, web_data, api_data):
        """Webã¨APIãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦æœ€çµ‚æ¨å¥¨ã‚’æ±ºå®š"""
        recommendation = {
            'priority_source': 'unknown',
            'summary': 'no_data',
            'warnings_detected': False,
            'time_info': 'unknown'
        }
        
        web_warnings = 0
        api_warnings = 0
        
        # Webè­¦å ±ã‚«ã‚¦ãƒ³ãƒˆ
        if web_data and 'warnings' in web_data:
            web_warnings = len(web_data['warnings'])
        
        # APIè­¦å ±ã‚«ã‚¦ãƒ³ãƒˆ
        if api_data and 'warnings' in api_data:
            api_warnings = len(api_data['warnings'])
        
        # å„ªå…ˆé †ä½æ±ºå®š
        if web_warnings > 0 and api_warnings > 0:
            recommendation.update({
                'priority_source': 'web_priority',
                'summary': f'Web: {web_warnings}ä»¶, API: {api_warnings}ä»¶ â†’ Webå„ªå…ˆ',
                'warnings_detected': True,
                'recommended_data': web_data
            })
        elif web_warnings > 0:
            recommendation.update({
                'priority_source': 'web_only',
                'summary': f'Webã®ã¿è­¦å ±æ¤œå‡º: {web_warnings}ä»¶',
                'warnings_detected': True,
                'recommended_data': web_data
            })
        elif api_warnings > 0:
            recommendation.update({
                'priority_source': 'api_only',
                'summary': f'APIã®ã¿è­¦å ±æ¤œå‡º: {api_warnings}ä»¶',
                'warnings_detected': True,
                'recommended_data': api_data
            })
        else:
            recommendation.update({
                'priority_source': 'no_warnings',
                'summary': 'è­¦å ±ãƒ»æ³¨æ„å ±ãªã—',
                'warnings_detected': False
            })
        
        # æ™‚åˆ»æƒ…å ±
        if api_data and 'report_datetime' in api_data:
            recommendation['time_info'] = api_data['report_datetime']
        
        return recommendation
    
    def display_summary(self, results):
        """çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“Š === åˆ†æçµæœã‚µãƒãƒªãƒ¼ ===")
        
        for area_code, area_result in results['areas'].items():
            area_name = area_result['area_name']
            final_rec = area_result['final_recommendation']
            
            print(f"\nğŸ® {area_name}")
            print(f"   å„ªå…ˆã‚½ãƒ¼ã‚¹: {final_rec.get('priority_source', 'unknown')}")
            print(f"   ã‚µãƒãƒªãƒ¼: {final_rec.get('summary', 'unknown')}")
            print(f"   è­¦å ±æ¤œå‡º: {'ã‚ã‚Š' if final_rec.get('warnings_detected') else 'ãªã—'}")
            print(f"   ç™ºè¡¨æ™‚åˆ»: {final_rec.get('time_info', 'unknown')}")

if __name__ == "__main__":
    scraper = JMAAdvancedWebScraper()
    result = scraper.run_comprehensive_analysis()