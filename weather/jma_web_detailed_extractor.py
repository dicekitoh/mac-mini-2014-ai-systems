#!/usr/bin/env python3
"""
æ°—è±¡åºWebã‚µã‚¤ãƒˆã‹ã‚‰HTMLå†…ã®ç™ºè¡¨æ™‚åˆ»ã¨è­¦å ±è©³ç´°ã‚’æŠ½å‡º
"""

import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

class JMAWebDetailExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.area_codes = {
            '016000': 'çŸ³ç‹©åœ°æ–¹',
            '015000': 'ç©ºçŸ¥åœ°æ–¹'
        }
    
    def extract_web_details(self, area_code):
        """Webãƒšãƒ¼ã‚¸ã‹ã‚‰HTMLè©³ç´°ã‚’æŠ½å‡º"""
        area_name = self.area_codes[area_code]
        url = f"https://www.jma.go.jp/bosai/warning/#area_type=class20s&area_code={area_code}"
        
        print(f"\nğŸ” {area_name} Webè©³ç´°æŠ½å‡ºä¸­...")
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            full_text = soup.get_text()
            
            # æ™‚åˆ»ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©³ç´°æ¤œç´¢
            time_patterns = [
                (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2})æ™‚(\d{1,2})åˆ†ç™ºè¡¨', 'å¹´æœˆæ—¥æ™‚åˆ†ç™ºè¡¨'),
                (r'(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2})æ™‚(\d{1,2})åˆ†ç™ºè¡¨', 'æœˆæ—¥æ™‚åˆ†ç™ºè¡¨'),
                (r'(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2})æ™‚(\d{1,2})åˆ†', 'æœˆæ—¥æ™‚åˆ†'),
                (r'(\d{1,2})æ—¥(\d{1,2})æ™‚(\d{1,2})åˆ†ç™ºè¡¨', 'æ—¥æ™‚åˆ†ç™ºè¡¨'),
                (r'(\d{1,2})æ™‚(\d{1,2})åˆ†ç™ºè¡¨', 'æ™‚åˆ†ç™ºè¡¨'),
                (r'ä»¤å’Œ(\d+)å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥(\d{1,2})æ™‚(\d{1,2})åˆ†', 'ä»¤å’Œå¹´æœˆæ—¥æ™‚åˆ†')
            ]
            
            found_times = []
            for pattern, description in time_patterns:
                matches = re.finditer(pattern, full_text)
                for match in matches:
                    start = max(0, match.start() - 100)
                    end = min(len(full_text), match.end() + 100)
                    context = full_text[start:end].strip()
                    
                    found_times.append({
                        'time_text': match.group(),
                        'pattern_type': description,
                        'context': context,
                        'position': match.start()
                    })
            
            # è­¦å ±ãƒ»æ³¨æ„å ±ã®è©³ç´°æ¤œç´¢
            warning_patterns = [
                (r'([^ã€‚\n]*)(æ¿ƒéœ§|æš´é¢¨|å¤§é›¨|æ´ªæ°´|å¤§é›ª|æš´é¢¨é›ª|é›·|å¼·é¢¨|ä¹¾ç‡¥|ãªã ã‚Œ|ç€æ°·|ç€é›ª|èé›ª|éœœ|ä½æ¸©)(è­¦å ±|æ³¨æ„å ±)([^ã€‚\n]*)', 'è­¦å ±æ³¨æ„å ±è©³ç´°'),
                (r'([^ã€‚\n]*)(çŸ³ç‹©|ç©ºçŸ¥)([^ã€‚\n]*)(è­¦å ±|æ³¨æ„å ±)([^ã€‚\n]*)', 'åœ°åŸŸåˆ¥è­¦å ±'),
                (r'(ç™ºè¡¨|ç¶™ç¶š|è§£é™¤|è­¦æˆ’|æ³¨æ„)([^ã€‚\n]*)', 'çŠ¶æ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰')
            ]
            
            found_warnings = []
            for pattern, description in warning_patterns:
                matches = re.finditer(pattern, full_text)
                for match in matches:
                    start = max(0, match.start() - 50)
                    end = min(len(full_text), match.end() + 50)
                    context = full_text[start:end].strip()
                    
                    found_warnings.append({
                        'warning_text': match.group(),
                        'pattern_type': description,
                        'context': context,
                        'position': match.start()
                    })
            
            # HTMLã‚¿ã‚°ã‹ã‚‰æ§‹é€ çš„æƒ…å ±ã‚’æŠ½å‡º
            structural_info = self.extract_structural_info(soup)
            
            return {
                'area_name': area_name,
                'area_code': area_code,
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'url': url,
                'found_times': found_times,
                'found_warnings': found_warnings,
                'structural_info': structural_info,
                'total_time_matches': len(found_times),
                'total_warning_matches': len(found_warnings),
                'page_title': soup.title.get_text(strip=True) if soup.title else 'ä¸æ˜'
            }
            
        except Exception as e:
            return {
                'area_name': area_name,
                'area_code': area_code,
                'error': str(e),
                'access_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
    
    def extract_structural_info(self, soup):
        """HTMLã®æ§‹é€ çš„æƒ…å ±ã‚’æŠ½å‡º"""
        structural = {
            'meta_tags': [],
            'script_content': [],
            'data_attributes': []
        }
        
        # metaã‚¿ã‚°ã‹ã‚‰æƒ…å ±æŠ½å‡º
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('name') or meta.get('property'):
                content = meta.get('content', '')
                if any(keyword in content.lower() for keyword in ['weather', 'warning', 'alert', 'meteorological']):
                    structural['meta_tags'].append({
                        'name': meta.get('name') or meta.get('property'),
                        'content': content
                    })
        
        # data-*å±æ€§ã‚’æŒã¤è¦ç´ 
        data_elements = soup.find_all(attrs=lambda x: x and any(key.startswith('data-') for key in x.keys()))
        for element in data_elements[:10]:  # æœ€åˆã®10å€‹
            data_attrs = {k: v for k, v in element.attrs.items() if k.startswith('data-')}
            if data_attrs:
                structural['data_attributes'].append({
                    'tag': element.name,
                    'data_attrs': data_attrs,
                    'text': element.get_text(strip=True)[:100]  # æœ€åˆã®100æ–‡å­—
                })
        
        return structural
    
    def run_detailed_analysis(self):
        """è©³ç´°åˆ†æå®Ÿè¡Œ"""
        print("ğŸŒ¤ï¸  æ°—è±¡åºWebè©³ç´°æŠ½å‡ºã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 40)
        
        results = {
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'areas': {}
        }
        
        for area_code, area_name in self.area_codes.items():
            result = self.extract_web_details(area_code)
            results['areas'][area_code] = result
            
            # çµæœè¡¨ç¤º
            self.display_result(result)
        
        return results
    
    def display_result(self, result):
        """çµæœè¡¨ç¤º"""
        if 'error' in result:
            print(f"âŒ {result['area_name']}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")
            return
        
        area_name = result['area_name']
        print(f"\nğŸ“Š {area_name} Webè©³ç´°çµæœ:")
        
        # ç™ºè¡¨æ™‚åˆ»æƒ…å ±
        times = result.get('found_times', [])
        print(f"  ğŸ• ç™ºè¡¨æ™‚åˆ»: {len(times)}ä»¶æ¤œå‡º")
        for i, time_info in enumerate(times[:5]):  # æœ€åˆã®5ä»¶
            print(f"    {i+1}. {time_info['time_text']} ({time_info['pattern_type']})")
            print(f"       å‰å¾Œæ–‡è„ˆ: {time_info['context'][:80]}...")
        
        # è­¦å ±æƒ…å ±
        warnings = result.get('found_warnings', [])
        print(f"  âš ï¸  è­¦å ±è©³ç´°: {len(warnings)}ä»¶æ¤œå‡º")
        for i, warning_info in enumerate(warnings[:3]):  # æœ€åˆã®3ä»¶
            print(f"    {i+1}. {warning_info['pattern_type']}")
            print(f"       å†…å®¹: {warning_info['warning_text'][:100]}...")
        
        # æ§‹é€ æƒ…å ±
        structural = result.get('structural_info', {})
        meta_count = len(structural.get('meta_tags', []))
        data_count = len(structural.get('data_attributes', []))
        print(f"  ğŸ—ï¸  æ§‹é€ æƒ…å ±: meta {meta_count}ä»¶, dataå±æ€§ {data_count}ä»¶")

if __name__ == "__main__":
    extractor = JMAWebDetailExtractor()
    result = extractor.run_detailed_analysis()